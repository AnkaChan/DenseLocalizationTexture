{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anka\\.conda\\envs\\ChbCapture\\lib\\site-packages\\dask\\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    }
   ],
   "source": [
    "import cv2, glob, itertools, json\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import os, tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from feature_extractor import MobileNet, Resnet, Vgg16\n",
    "from modules import atrous_spatial_pyramid_pooling\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UVExtractor(object):\n",
    "    def __init__(self, base_architecture, training=True, ignore_label=0,batch_norm_momentum=0.9997, pre_trained_model=None, log_dir='./' ):\n",
    "        tf.reset_default_graph() \n",
    "        \n",
    "        self.is_training = tf.placeholder(tf.bool, None, name='is_training')\n",
    "        self.ignore_label = ignore_label\n",
    "        self.inputs_shape = [None, 60, 60, 3]\n",
    "        self.labels_shape = [None, 60, 60, 2]\n",
    "        self.training = training\n",
    "        self.inputs = tf.placeholder(tf.float32, shape=self.inputs_shape, name='inputs')\n",
    "        self.labels = tf.placeholder(tf.float32, shape=self.labels_shape, name='labels')\n",
    "\n",
    "        self.target_height = tf.placeholder(tf.int32, None, name='target_image_height')\n",
    "        self.target_width = tf.placeholder(tf.int32, None, name='target_image_width')\n",
    "\n",
    "        self.weight_decay = tf.placeholder(tf.float32, None, name='weight_decay')\n",
    "        self.regularizer = tf.contrib.layers.l2_regularizer(scale=self.weight_decay)\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "\n",
    "        self.feature_map = self.backbone_initializer(base_architecture)\n",
    "        if pre_trained_model:\n",
    "            self.initialize_backbone_from_pretrained_weights(pre_trained_model)\n",
    "        self.outputs, self.outputs_resized = self.model_initializer()\n",
    "\n",
    "        self.learning_rate = tf.placeholder(tf.float32, None, name='learning_rate')\n",
    "        self.loss = self.loss_initializer()\n",
    "        self.optimizer = self.optimizer_initializer()\n",
    "\n",
    "        # Initialize tensorflow session\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        if self.training:\n",
    "            self.train_step = 0\n",
    "            now = datetime.now()\n",
    "            self.log_dir = os.path.join(log_dir, now.strftime('%Y%m%d-%H%M%S'))\n",
    "            self.writer = tf.summary.FileWriter(self.log_dir, tf.get_default_graph())\n",
    "            self.train_summaries, self.valid_summaries = self.summary()\n",
    "\n",
    "    def backbone_initializer(self, base_architecture):\n",
    "\n",
    "        with tf.variable_scope('backbone'):\n",
    "            if base_architecture == 'vgg16':\n",
    "                features = Vgg16(self.inputs, self.weight_decay, self.batch_norm_momentum)\n",
    "            elif base_architecture.startswith('resnet'):\n",
    "                n_layers = int(base_architecture.split('_')[-1])\n",
    "                features = Resnet(n_layers, self.inputs, self.weight_decay, self.batch_norm_momentum, self.is_training)\n",
    "            elif base_architecture.startswith('mobilenet'):\n",
    "                depth_multiplier = float(base_architecture.split('_')[-1])\n",
    "                features = MobileNet(depth_multiplier, self.inputs, self.weight_decay, self.batch_norm_momentum, self.is_training)\n",
    "            else:\n",
    "                raise ValueError('Unknown backbone architecture!')\n",
    "\n",
    "        return features\n",
    "\n",
    "    def model_initializer(self):\n",
    "\n",
    "        pools = atrous_spatial_pyramid_pooling(inputs=self.feature_map, filters=256, regularizer=self.regularizer)\n",
    "        logits = tf.layers.conv2d(inputs=pools, filters=2, kernel_size=(1, 1), name='logits')\n",
    "#         outputs = tf.image.resize_bilinear(images=logits, size=(self.target_height, self.target_width), name='resized_outputs')\n",
    "        outputs = logits\n",
    "        outputs_resized = tf.image.resize_bilinear(images=logits, size=(self.target_height, self.target_width), name='resized_outputs')\n",
    "        return outputs, outputs_resized\n",
    "\n",
    "    def loss_initializer(self):\n",
    "    \n",
    "#         labels_linear = tf.reshape(tensor=self.labels, shape=[-1])\n",
    "#         not_ignore_mask = tf.to_float(tf.not_equal(labels_linear, self.ignore_label))\n",
    "        # The locations represented by indices in indices take value on_value, while all other locations take value off_value.\n",
    "        # For example, ignore label 255 in VOC2012 dataset will be set to zero vector in onehot encoding (looks like the not ignore mask is not required)\n",
    "        # onehot_labels = tf.one_hot(indices=labels_linear, depth=, on_value=1.0, off_value=0.0)\n",
    "    \n",
    "        # loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=tf.reshape(self.outputs, shape=[-1, self.num_classes]), weights=not_ignore_mask)\n",
    "        \n",
    "        loss = self.Smooth_l1_loss(self.labels, self.outputs_resized)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def optimizer_initializer(self):\n",
    "\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def initialize_backbone_from_pretrained_weights(self, path_to_pretrained_weights):\n",
    "\n",
    "        variables_to_restore = tf.contrib.slim.get_variables_to_restore(exclude=['global_step'])\n",
    "        valid_prefix = 'backbone/'\n",
    "        tf.train.init_from_checkpoint(path_to_pretrained_weights, {v.name[len(valid_prefix):].split(':')[0]: v for v in variables_to_restore if v.name.startswith(valid_prefix)})\n",
    "    \n",
    "    def Smooth_l1_loss(self, labels,predictions,scope=tf.GraphKeys.LOSSES):\n",
    "        with tf.variable_scope(scope):\n",
    "            diff=tf.abs(labels-predictions)\n",
    "            less_than_one=tf.cast(tf.less(diff,1.0),tf.float32)   #Bool to float32\n",
    "            smooth_l1_loss=(less_than_one*0.5*diff**2)+(1.0-less_than_one)*(diff-0.5)#同上图公式\n",
    "            return tf.reduce_mean(smooth_l1_loss)\n",
    "\n",
    "    def close(self):\n",
    "\n",
    "        if self.training:\n",
    "            self.writer.close()\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFolder = r'F:\\WorkingCopy2\\2021_RandomlyDeformedGridMesh\\Data'\n",
    "numMarkers = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsPerMaker = []\n",
    "uvsPerMaker = []\n",
    "\n",
    "for iM in range(numMarkers):\n",
    "    outImgDataFile = join(dataFolder, 'ImgMarker_' + str(iM).zfill(3) + '.npy')\n",
    "    outUVDataFile = join(dataFolder, 'UVMarker_' + str(iM).zfill(3) + '.npy')\n",
    "\n",
    "    img = np.load(outImgDataFile)\n",
    "    uv = np.load(outUVDataFile)\n",
    "    \n",
    "    imgsPerMaker.append(img)\n",
    "    uvsPerMaker.append(uv)\n",
    "\n",
    "#     print(img.shape)\n",
    "#     print(uv.shape)\n",
    "\n",
    "#     for i in range(50):\n",
    "#         iImg = np.random.randint(0, img.shape[0])\n",
    "#         cv2.imshow('crop', img[iImg, :, :])\n",
    "#         cv2.imshow('u', uv[iImg, :,:,0])\n",
    "#         cv2.imshow('v', uv[iImg, :,:,1])\n",
    "#         cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uvExtractor = UVExtractor('resnet_50', training=False,)\n",
    "# uvExtractor = UVExtractor('vgg16', training=False,)\n",
    "uvExtractor = UVExtractor('resnet_101', training=False,  pre_trained_model=r'C:\\Code\\MyRepo\\00_DeepLearning\\DeepLab-V3\\data\\models\\pretrained\\resnet_101\\resnet_v2_101.ckpt')\n",
    "# uvExtractor = UVExtractor('mobilenet_1.0_224', training=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 60, 60, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgsPerMaker[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4, 4, 2)\n"
     ]
    }
   ],
   "source": [
    "fd = {uvExtractor.inputs:imgsPerMaker[0][:10, :,:,:],\n",
    "      uvExtractor.is_training:False,\n",
    "      uvExtractor.target_width:60,\n",
    "      uvExtractor.target_height:60,\n",
    "      uvExtractor.labels:uvsPerMaker[0][:10, ...]\n",
    "     }\n",
    "output = uvExtractor.sess.run(uvExtractor.outputs, feed_dict=fd)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60, 60, 2)\n"
     ]
    }
   ],
   "source": [
    "output_resized = uvExtractor.sess.run(uvExtractor.outputs_resized, feed_dict=fd)\n",
    "print(output_resized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-547.1964  ,  137.62387 ],\n",
       "        [-538.4445  ,  134.9598  ],\n",
       "        [-529.6926  ,  132.29572 ],\n",
       "        ...,\n",
       "        [-500.0861  ,   94.04289 ],\n",
       "        [-500.0861  ,   94.04289 ],\n",
       "        [-500.0861  ,   94.04289 ]],\n",
       "\n",
       "       [[-555.53754 ,  128.42982 ],\n",
       "        [-546.90063 ,  125.628235],\n",
       "        [-538.26373 ,  122.82664 ],\n",
       "        ...,\n",
       "        [-501.40018 ,   91.9568  ],\n",
       "        [-501.40018 ,   91.9568  ],\n",
       "        [-501.40018 ,   91.9568  ]],\n",
       "\n",
       "       [[-563.87866 ,  119.23578 ],\n",
       "        [-555.35675 ,  116.29667 ],\n",
       "        [-546.8348  ,  113.35756 ],\n",
       "        ...,\n",
       "        [-502.71426 ,   89.870705],\n",
       "        [-502.71426 ,   89.870705],\n",
       "        [-502.71426 ,   89.870705]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-412.7161  ,  -81.54241 ],\n",
       "        [-411.09613 ,  -83.90189 ],\n",
       "        [-409.47617 ,  -86.26137 ],\n",
       "        ...,\n",
       "        [-441.0507  ,   20.698822],\n",
       "        [-441.0507  ,   20.698822],\n",
       "        [-441.0507  ,   20.698822]],\n",
       "\n",
       "       [[-412.7161  ,  -81.54241 ],\n",
       "        [-411.09613 ,  -83.90189 ],\n",
       "        [-409.47617 ,  -86.26137 ],\n",
       "        ...,\n",
       "        [-441.0507  ,   20.698822],\n",
       "        [-441.0507  ,   20.698822],\n",
       "        [-441.0507  ,   20.698822]],\n",
       "\n",
       "       [[-412.7161  ,  -81.54241 ],\n",
       "        [-411.09613 ,  -83.90189 ],\n",
       "        [-409.47617 ,  -86.26137 ],\n",
       "        ...,\n",
       "        [-441.0507  ,   20.698822],\n",
       "        [-441.0507  ,   20.698822],\n",
       "        [-441.0507  ,   20.698822]]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_resized[0,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureMap = uvExtractor.sess.run(uvExtractor.outputs, feed_dict=fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4, 4, 2)\n"
     ]
    }
   ],
   "source": [
    "print(featureMap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284.89804\n"
     ]
    }
   ],
   "source": [
    "print(uvExtractor.sess.run(uvExtractor.loss, feed_dict=fd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = {uvExtractor.inputs:imgsPerMaker[0][:10, :,:,:],\n",
    "      uvExtractor.is_training:True,\n",
    "      uvExtractor.target_width:60,\n",
    "      uvExtractor.target_height:60,\n",
    "      uvExtractor.labels:uvsPerMaker[0][:10, ...],\n",
    "      uvExtractor.weight_decay: 5e-4,\n",
    "      uvExtractor.learning_rate: 1e-5\n",
    "     }\n",
    "_, outputs, train_loss,  = uvExtractor.sess.run([uvExtractor.optimizer, uvExtractor.outputs, uvExtractor.loss,], feed_dict=fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4221286"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
